{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Exam 4th of January 2024, 8.00-13.00 for the course 1MS041 (Introduction to Data Science / Introduktion till dataanalys)\n",
    "\n",
    "## Instructions:\n",
    "1. Complete the problems by following instructions.\n",
    "2. When done, submit this file with your solutions saved, following the instruction sheet.\n",
    "\n",
    "This exam has 3 problems for a total of 40 points, to pass you need\n",
    "20 points. The bonus will be added to the score of the exam and rounded afterwards.\n",
    "\n",
    "## Some general hints and information:\n",
    "* Try to answer all questions even if you are uncertain.\n",
    "* Comment your code, so that if you get the wrong answer I can understand how you thought\n",
    "this can give you some points even though the code does not run.\n",
    "* Follow the instruction sheet rigorously.\n",
    "* This exam is partially autograded, but your code and your free text answers are manually graded anonymously.\n",
    "* If there are any questions, please ask the exam guards, they will escalate it to me if necessary.\n",
    "\n",
    "## Tips for free text answers\n",
    "* Be VERY clear with your reasoning, there should be zero ambiguity in what you are referring to.\n",
    "* If you want to include math, you can write LaTeX in the Markdown cells, for instance `$f(x)=x^2$` will be rendered as $f(x)=x^2$ and `$$f(x) = x^2$$` will become an equation line, as follows\n",
    "$$f(x) = x^2$$\n",
    "Another example is `$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$` which renders as\n",
    "$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$\n",
    "\n",
    "## Finally some rules:\n",
    "* You may not communicate with others during the exam, for example:\n",
    "    * You cannot ask for help in Stack-Overflow or other such help forums during the Exam.\n",
    "    * You may not communicate with AI's, for instance ChatGPT.\n",
    "    * Your on-line and off-line activity is being monitored according to the examination rules.\n",
    "\n",
    "## Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Insert your anonymous exam ID as a string in the variable below\n",
    "examID=\"0019-BYZ\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "In this problem you will do rejection sampling from complicated distributions, you will also be using your samples to compute certain integrals, a method known as Monte Carlo integration: (Keep in mind that choosing a good sampling distribution is often key to avoid too much rejection)\n",
    "\n",
    "1. [4p] Fill in the remaining part of the function `problem1_inversion` in order to produce samples from the below distribution using rejection sampling:\n",
    "\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        \\frac{e^{x^2}-1}{e-1}, & 0 < x < 1 \\\\\n",
    "        1, & x \\geq 1\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "2. [2p] Produce 100000 samples (**use fewer if it times-out and you cannot find a solution**) and put the answer in `problem1_samples` from the above distribution and plot the histogram together with the true density. *(There is a timeout decorator on this function and if it takes more than 10 seconds to generate 100000 samples it will timeout and it will count as if you failed to generate.)*\n",
    "3. [2p] Use the above 100000 samples (`problem1_samples`) to approximately compute the integral\n",
    "\n",
    "$$\n",
    "    \\int_0^{1} \\sin(x) \\frac{2e^{x^2} x}{e-1} dx\n",
    "$$\n",
    "and store the result in `problem1_integral`.\n",
    "\n",
    "4. [2p] Use Hoeffdings inequality to produce a 95\\% confidence interval of the integral above and store the result as a tuple in the variable `problem1_interval`\n",
    "\n",
    "5. [4p] Fill in the remaining part of the function `problem1_inversion_2` in order to produce samples from the below distribution using rejection sampling:\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        20xe^{20-1/x}, & 0 < x < \\frac{1}{20} \\\\\n",
    "        1, & x \\geq \\frac{1}{20}\n",
    "    \\end{cases}\n",
    "$$\n",
    "Hint: this is tricky because if you choose the wrong sampling distribution you reject at least 9 times out of 10. You will get points based on how long your code takes to create a certain number of samples, if you choose the correct sampling distribution you can easily create 100000 samples within 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Part 1\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "def problem1_inversion(n_samples=1):\n",
    "    # Distribution from part 1\n",
    "    # write the code in this function to produce samples from the distribution in the assignment\n",
    "    # Make sure you choose a good sampling distribution to avoid unnecessary rejections\n",
    "    problem1_samples = []\n",
    "\n",
    "    while len(problem1_samples) < n_samples:\n",
    "        x = np.random.uniform(0,1)       \n",
    "        # Calculate the probability of accepting the sample\n",
    "        f_x = (np.exp(x**2) / ((np.exp(1))-1))\n",
    "\n",
    "        # Use the next seed value for the acceptance check\n",
    "        u = np.random.uniform(0,1)  \n",
    "        # Increment the seed for the next iteration\n",
    "\n",
    "        # Accept the sample with probability f(x)/(M*g(x))\n",
    "        if u < f_x:\n",
    "            problem1_samples.append(x)\n",
    "    \n",
    "    return np.array(problem1_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "import numpy as np\n",
    "c=problem1_inversion(n_samples=100000)\n",
    "from Utils import discrete_histogram\n",
    "discrete_histogram(c,normed=False,alpha=1)\n",
    "#problem1_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27451961599495833\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "import math\n",
    "import numpy as np\n",
    "def MC(samples,n_samples): \n",
    "    integral_value = 0\n",
    "    for i in range(n_samples):\n",
    "        x = samples[i]\n",
    "        integral_value = integral_value + (1-0)*(math.sin(x) * 2 * np.exp(x**2)*x )/(np.exp(1) -1)\n",
    "    integral_value = integral_value/n_samples\n",
    "    return integral_value\n",
    "problem1_integral = MC(problem1_samples,len(problem1_samples))\n",
    "print(problem1_integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2654390296920522, 0.27402841785898696)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 4\n",
    "from numpy import *\n",
    "def hoeffding_interval(sample_mean, n, a, b, confidence_level=0.95):\n",
    "    epsilon = np.sqrt(np.log(2 / (1 - confidence_level)) / (2 * n))\n",
    "    return max(a, sample_mean - epsilon * (b - a)), min(b, sample_mean + epsilon * (b - a))\n",
    "problem1_samples_avg = np.mean(problem1_samples)\n",
    "n = len(problem1_samples)\n",
    "a = 0.0 \n",
    "b = 1.0\n",
    "problem1_interval = hoeffding_interval(problem1_samples_avg,n,a,b)\n",
    "problem1_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [],
   "source": [
    "# Part 5\n",
    "import random\n",
    "import numpy as np\n",
    "def problem1_inversion_2(n_samples=1):\n",
    "    # Distribution from part 2\n",
    "    # write the code in this function to produce samples from the distribution in the assignment\n",
    "    # Make sure you choose a good sampling distribution to avoid unnecessary rejections\n",
    "    samples = [ ]\n",
    "    for t in range(n_samples):\n",
    "        y = random.random()\n",
    "        if y <= 0:\n",
    "            x = 0\n",
    "            samples.append(x)\n",
    "        elif y > 0 and y < 1/20:\n",
    "            x = 20 * x * np.exp(20- 1/x )\n",
    "            samples.append(x)\n",
    "        else:\n",
    "            x = 1\n",
    "            samples.append(x)\n",
    "    # Return a numpy array of length n_samples\n",
    "    return np.asarray(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "#### Local Test for Exam vB, PROBLEM 1\n",
    "Evaluate cell below to make sure your answer is valid.                             You **should not** modify anything in the cell below when evaluating it to do a local test of                             your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good, your problem1_inversion returns a numpy array\n",
      "Good, your problem1_samples is a numpy array\n",
      "Good, your problem1_integral is a float\n",
      "Good, your problem1_interval is a tuple or list of length 2\n",
      "Good, your problem1_inversion_2 returns a numpy array\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This cell is just to check that you got the correct formats of your answer\n",
    "import numpy as np\n",
    "try:\n",
    "    assert(isinstance(problem1_inversion(10), np.ndarray)) \n",
    "except:\n",
    "    print(\"Try again. You should return a numpy array from problem1_inversion\")\n",
    "else:\n",
    "    print(\"Good, your problem1_inversion returns a numpy array\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_samples, np.ndarray)) \n",
    "except:\n",
    "    print(\"Try again. your problem1_samples is not a numpy array\")\n",
    "else:\n",
    "    print(\"Good, your problem1_samples is a numpy array\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_integral, float)) \n",
    "except:\n",
    "    print(\"Try again. your problem1_integral is not a float\")\n",
    "else:\n",
    "    print(\"Good, your problem1_integral is a float\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_interval, list) or isinstance(problem1_interval, tuple)) , \"problem1_interval not a tuple or list\"\n",
    "    assert(len(problem1_interval) == 2) , \"problem1_interval does not have length 2, it should have a lower bound and an upper bound\"\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"Good, your problem1_interval is a tuple or list of length 2\")\n",
    "\n",
    "try:\n",
    "    assert(isinstance(problem1_inversion_2(10), np.ndarray)) \n",
    "except:\n",
    "    print(\"Try again. You should return a numpy array from problem1_inversion_2\")\n",
    "else:\n",
    "    print(\"Good, your problem1_inversion_2 returns a numpy array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "source": [
    "\n",
    "Let us build a proportional model ($\\mathbb{P}(Y=1 \\mid X) = G(\\beta_0+\\beta \\cdot X)$ where $G$ is the logistic function) for the spam vs not spam data. Here we assume that the features are presence vs not presence of a word, let $X_1,X_2,X_3$ denote the presence (1) or absence (0) of the words $(\"free\", \"prize\", \"win\")$.\n",
    "\n",
    "1. [2p] Load the file `data/spam.csv` and create two numpy arrays, `problem2_X` which has shape (n_emails,3) where each feature in `problem2_X` corresponds to $X_1,X_2,X_3$ from above, `problem2_Y` which has shape **(n_emails,)** and consists of a $1$ if the email is spam and $0$ if it is not. Split this data into a train-calibration-test sets where we have the split $40\\%$, $20\\%$, $40\\%$, put this data in the designated variables in the code cell.\n",
    "\n",
    "2. [4p] Follow the calculation from the lecture notes where we derive the logistic regression and implement the final loss function inside the class `ProportionalSpam`. You can use the `Test` cell to check that it gives the correct value for a test-point.\n",
    "\n",
    "3. [4p] Train the model `problem2_ps` on the training data. The goal is to calibrate the probabilities output from the model. Start by creating a new variable `problem2_X_pred` (shape `(n_samples,1)`) which consists of the predictions of `problem2_ps` on the calibration dataset. Then train a calibration model using `sklearn.tree.DecisionTreeRegressor`, store this trained model in `problem2_calibrator`.\n",
    "\n",
    "4. [3p] Use the trained model `problem2_ps` and the calibrator `problem2_calibrator` to make final predictions on the testing data, store the prediction in `problem2_final_predictions`. Compute the $0-1$ test-loss and store it in `problem2_01_loss` and provide a $99\\%$ confidence interval of it, store this in the variable `problem2_interval`, this should again be a tuple as in **problem1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2228, 3) (1115, 3) (2229, 3) (2228,) (1115,) (2229,)\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "def load_sms():\n",
    "    import csv\n",
    "    lines = []\n",
    "    hamspam = {'ham': 0, 'spam': 1}\n",
    "    with open('data/spam.csv', mode='r',encoding='latin-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        lines = [(line[1],hamspam[line[0]]) for line in reader]\n",
    "        \n",
    "    return lines\n",
    "spam_no_spam = load_sms()\n",
    "#print(spam_no_spam)\n",
    "x1 = [ ]\n",
    "x2 = [ ]\n",
    "x3 = [ ]\n",
    "def getx(dataset,keyword):\n",
    "    x = []\n",
    "\n",
    "    for sms, is_spam in dataset:\n",
    "        # Splitting the SMS into words\n",
    "        words = sms.lower().split()\n",
    "        # Check if 'free' or 'prize' are present as complete words\n",
    "        has_keywords = keyword in words\n",
    "\n",
    "\n",
    "        if has_keywords:\n",
    "            x.append('1')\n",
    "        else:\n",
    "            x.append('0')\n",
    "    return x\n",
    "x1 = getx(spam_no_spam,\"free\")\n",
    "x2 = getx(spam_no_spam,\"prize\")\n",
    "x3 = getx(spam_no_spam,\"win\")\n",
    "problem2_X = np.array([x1,x2,x3])\n",
    "problem2_X = problem2_X.transpose()\n",
    "def getY(dataset):\n",
    "    y = [ ]\n",
    "    for sms, is_spam in dataset:\n",
    "        if is_spam:\n",
    "            y.append('1')\n",
    "        else:\n",
    "            y.append('0')\n",
    "    return y\n",
    "Y = getY(spam_no_spam)\n",
    "Y = np.array(Y) \n",
    "problem2_Y = Y\n",
    "#print(problem2_X.shape)\n",
    "#print(problem2_Y.shape)\n",
    "def train_test_validation(X,Y,test_size=0.2,validation_size=0.2,random_state=None,shuffle=True):\n",
    "    \"\"\"\n",
    "    Performs a train test validation split of the data [train_data,test_data,validation_data]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : The input X, shape (n_samples,n_features)\n",
    "    Y : The input labells, shape (n_samples)\n",
    "    test_size : the proportion of data that should be test data\n",
    "    validation_size : the proportion of data that should be validation data\n",
    "    random_state : the random state variable passed through to sklearns train_test_split\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    X_train, X_test, X_valid, Y_train, Y_test, Y_valid\n",
    "\n",
    "    Examples:\n",
    "    ----------\n",
    "    >>> X_train, X_test, X_valid, Y_train, Y_test, Y_valid = train_test_validation(X,Y,test_size=0.25,validation_size=0.25)\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train,X_tt,Y_train,Y_tt = train_test_split(X,Y,\n",
    "                                                 test_size=test_size+validation_size,\n",
    "                                                 random_state=random_state,\n",
    "                                                 shuffle=shuffle)\n",
    "    X_test,X_valid,Y_test,Y_valid = train_test_split(X_tt,Y_tt,\n",
    "                                                     test_size=(validation_size)/(test_size + validation_size),\n",
    "                                                     random_state=random_state,\n",
    "                                                     shuffle=shuffle)\n",
    "\n",
    "    return X_train, X_test, X_valid, Y_train, Y_test, Y_valid\n",
    "\n",
    "#problem2_X_train = XXX\n",
    "#problem2_X_calib = XXX\n",
    "#problem2_X_test = XXX\n",
    "\n",
    "#problem2_Y_train = XXX\n",
    "#problem2_Y_calib = XXX\n",
    "#problem2_Y_test = XXX\n",
    "problem2_X_train,problem2_X_test,problem2_X_calib,problem2_Y_train,problem2_Y_test,problem2_Y_calib = train_test_validation(problem2_X,problem2_Y,test_size = 0.4,validation_size = 0.2)\n",
    "print(problem2_X_train.shape,problem2_X_calib.shape,problem2_X_test.shape,problem2_Y_train.shape,problem2_Y_calib.shape,problem2_Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "class ProportionalSpam(object):\n",
    "    def __init__(self):\n",
    "        self.coeffs = None\n",
    "        self.result = None\n",
    "    \n",
    "    # define the objective/cost/loss function we want to minimise\n",
    "    def loss(self,X,Y,coeffs):\n",
    "        #cost = np.transpose(-y)@np.log(h) - np.transpose(1-y)@np.log(1-h) + (l/2)*np.transpose(t[1:])@t[1:]\n",
    "        #cost = (1/m)*cost\n",
    "        #return cost\n",
    "        y_hat = self.predict(X)\n",
    "        lossvalue = -np.mean(Y*np.log(y_hat) + (1- Y )*np.log(1-y_hat))\n",
    "        return lossvalue\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        import numpy as np\n",
    "        from scipy import optimize\n",
    "\n",
    "        #Use the f above together with an optimization method from scipy\n",
    "        #to find the coefficients of the model\n",
    "        opt_loss = lambda coeffs: self.loss(X,Y,coeffs)\n",
    "        initial_arguments = np.zeros(shape=X.shape[1]+1)\n",
    "        self.result = optimize.minimize(opt_loss, initial_arguments,method='cg')\n",
    "        self.coeffs = self.result.x\n",
    "    \n",
    "    def predict(self,X):\n",
    "        #Use the trained model to predict Y\n",
    "        if (self.coeffs is not None):\n",
    "            G = lambda x: np.exp(x)/(1+np.exp(x))\n",
    "            return np.round(10*G(np.dot(X,self.coeffs[1:])+self.coeffs[0]))/10 # This rounding is to help you with the calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type NoneType which has no callable log method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'log'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Part 3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m problem2_ps \u001b[38;5;241m=\u001b[39m ProportionalSpam()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mproblem2_ps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem2_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproblem2_Y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m problem2_X_pred \u001b[38;5;241m=\u001b[39m problem2_ps\u001b[38;5;241m.\u001b[39mpredict(problem2_X_calib)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#train DecisonTreeRegressor\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m, in \u001b[0;36mProportionalSpam.fit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     23\u001b[0m opt_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m coeffs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(X,Y,coeffs)\n\u001b[0;32m     24\u001b[0m initial_arguments \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_arguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:689\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    687\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_powell(fun, x0, args, callback, bounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 689\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    691\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1686\u001b[0m, in \u001b[0;36m_minimize_cg\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m-> 1686\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1689\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[0;32m   1690\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    328\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(grad):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mProportionalSpam.fit.<locals>.<lambda>\u001b[1;34m(coeffs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#Use the f above together with an optimization method from scipy\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#to find the coefficients of the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m opt_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m coeffs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcoeffs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m initial_arguments \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(opt_loss, initial_arguments,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mProportionalSpam.loss\u001b[1;34m(self, X, Y, coeffs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,Y,coeffs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#cost = np.transpose(-y)@np.log(h) - np.transpose(1-y)@np.log(1-h) + (l/2)*np.transpose(t[1:])@t[1:]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#cost = (1/m)*cost\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#return cost\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m---> 14\u001b[0m     lossvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(Y\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m Y )\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_hat))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lossvalue\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type NoneType which has no callable log method"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "\n",
    "problem2_ps = ProportionalSpam()\n",
    "problem2_ps.fit(problem2_X_train,problem2_Y_train)\n",
    "problem2_X_pred = problem2_ps.predict(problem2_X_calib)\n",
    "\n",
    "\n",
    "#train DecisonTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "problem2_calibrator = DecisionTreeRegressor()\n",
    "problem2_calibrator.fit(problem2_X_train,problem2_Y_train)\n",
    "DecisionTreeRegressorproblem2_X_pred = problem2_calibrator.predict(problem2_X_calib)\n",
    "#print(DecisionTreeRegressorproblem2_X_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'subtract' did not contain a loop with signature matching types (dtype('float64'), dtype('<U1')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m problem2_final_predictions \u001b[38;5;241m=\u001b[39m problem2_calibrator\u001b[38;5;241m.\u001b[39mpredict(problem2_X_test)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# In order to compute this loss we first need to convert the predicted probabilities to a decision\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# recall the Bayes classifier?\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m problem2_01_loss \u001b[38;5;241m=\u001b[39m (\u001b[43mproblem2_final_predictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproblem2_Y_test\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Recall the interval is given as a tuple (a,b) or a list [a,b]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhoeffding_interval\u001b[39m(sample_mean, n, a, b, confidence_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m):\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types (dtype('float64'), dtype('<U1')) -> None"
     ]
    }
   ],
   "source": [
    "# Part 4\n",
    "import numpy as np\n",
    "# These are the predicted probabilities\n",
    "problem2_final_predictions = problem2_calibrator.predict(problem2_X_test)\n",
    "\n",
    "\n",
    "# In order to compute this loss we first need to convert the predicted probabilities to a decision\n",
    "# recall the Bayes classifier?\n",
    "problem2_01_loss = (problem2_final_predictions - problem2_Y_test).sum()\n",
    "\n",
    "# Recall the interval is given as a tuple (a,b) or a list [a,b]\n",
    "def hoeffding_interval(sample_mean, n, a, b, confidence_level=0.95):\n",
    "    epsilon = np.sqrt(np.log(2 / (1 - confidence_level)) / (2 * n))\n",
    "    return max(a, sample_mean - epsilon * (b - a)), min(b, sample_mean + epsilon * (b - a))\n",
    "sa_avg = np.mean(problem2_final_predictions)\n",
    "n = len(problem2_final_predictions)\n",
    "a = 0.0\n",
    "b = 1.0 \n",
    "problem2_interval = hoeffding_interval(sa_avg,n,a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "source": [
    "---\n",
    "#### Local Test for Exam vB, PROBLEM 2\n",
    "Evaluate cell below to make sure your answer is valid.                             You **should not** modify anything in the cell below when evaluating it to do a local test of                             your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "2",
    "lx_problem_points": "13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss was not correct on a test point\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    test_instance = ProportionalSpam()\n",
    "    test_loss = test_instance.loss(np.array([[1,0,1],[0,1,1]]),np.array([1,0]),np.array([1.2,0.4,0.3,0.9]))\n",
    "    assert (np.abs(test_loss-1.2828629432232497) < 1e-6)\n",
    "    print(\"Your loss was correct for a test point\")\n",
    "except:\n",
    "    print(\"Your loss was not correct on a test point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "source": [
    "\n",
    "Consider the following four Markov chains, answer each question for all chains:\n",
    "\n",
    "<img width=\"400px\" src=\"pictures/MarkovA.png\">Markov chain A</img>\n",
    "<img width=\"400px\" src=\"pictures/MarkovB.png\">Markov chain B</img>\n",
    "<img width=\"400px\" src=\"pictures/MarkovC.png\">Markov chain C</img>\n",
    "<img width=\"400px\" src=\"pictures/MarkovD.png\">Markov chain D</img>\n",
    "\n",
    "1. [2p] What is the transition matrix?\n",
    "2. [2p] Is the Markov chain irreducible?\n",
    "3. [3p] Is the Markov chain aperiodic? What is the period for each state?\n",
    "4. [3p] Does the Markov chain have a stationary distribution, and if so, what is it?\n",
    "5. [3p] Is the Markov chain reversible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 1\n",
    "\n",
    "#------------------------TRANSITION MATRIX -------------------------------\n",
    "# Answer each one by supplying the transition matrix as a numpy array\n",
    "# of shape (n_states,n_states), where state (A,B,...) corresponds to index (0,1,...)\n",
    "import numpy as np\n",
    "\n",
    "problem3_A = np.array([[0.8, 0.2, 0.0, 0.0],\n",
    "              [0.6, 0.2, 0.2, 0.0],\n",
    "              [0.0, 0.4, 0.0, 0.6],\n",
    "              [0.0, 0.0, 0.8, 0.2]])\n",
    "problem3_B   = np.array([[0.0,0.2,0.0,0.8],\n",
    "                          [0.0,0.0,1.0,0.0],\n",
    "                          [0.0,1.0,0.0,0.0],\n",
    "                          [0.5,0.0,0.5,0.0]])\n",
    "problem3_C   = np.array([[0.2,0.3,0.0,0.0,0.5],\n",
    "                          [0.2,0.2,0.6,0.0,0.0],\n",
    "                          [0.0,0.4,0.0,0.6,0.0],\n",
    "                          [0.0,0.0,0.0,0.6,0.4],\n",
    "                          [0.0,0.0,0.0,0.4,0.6]])\n",
    "problem3_D   = np.array([[0.8,0.2,0.0,0.0],\n",
    "                          [0.6,0.2,0.2,0.0],\n",
    "                          [0.0,0.4,0.0,0.6],\n",
    "                          [0.1,0.0,0.7,0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 2\n",
    "#------------------------REDUCIBLE -------------------------------\n",
    "# Answer each one with a True or False\n",
    "'''Irreducibility: a Markov chain is irreducible if its state space has only one connected class, \n",
    "i.e., the full membership of the state space,\n",
    "and the irreducibility of a Markov chain implies \n",
    "that the random variable can be transferred between any states during its evolution .'''\n",
    "problem3_A_irreducible = True\n",
    "problem3_B_irreducible = False\n",
    "problem3_C_irreducible = True\n",
    "problem3_D_irreducible = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 3\n",
    "#------------------------APERIODIC-------------------------------\n",
    "# Answer each one with a True or False\n",
    "'''Non-periodic: an MC,there does not exist a state from which the length of time elapsed \n",
    "before returning to this state is periodic, \n",
    "Theorem: Irreducible and non-periodic finite state Markov chains with a unique smooth distribution.'''\n",
    "problem3_A_is_aperiodic = True\n",
    "problem3_B_is_aperiodic = False\n",
    "problem3_C_is_aperiodic = False\n",
    "problem3_D_is_aperiodic = True\n",
    "\n",
    "# Answer the following with the period of the states as a numpy array\n",
    "# of shape (n_states,)\n",
    "\n",
    "problem3_A_periods = np.array([1,1,1,1])\n",
    "problem3_B_periods = np.array([1,1,1,1])\n",
    "problem3_C_periods = np.array([2,2,1,2,2])\n",
    "problem3_D_periods = np.array([2,2,2,2])\n",
    "#print(problem3_A_periods.shape)\n",
    "#print(problem3_B_periods.shape)\n",
    "#print(problem3_C_periods.shape)\n",
    "#print(problem3_D_periods.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 4\n",
    "import numpy as np\n",
    "def getstationary(P):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)  \n",
    "    stationary_distribution = np.real(eigenvectors[:, 0] / eigenvectors[:, 0].sum()) \n",
    "    answer_stationary = stationary_distribution\n",
    "    return answer_stationary\n",
    "def isver(P):\n",
    "    isnotinverse = 0\n",
    "    inverse =  1\n",
    "    import numpy as np\n",
    "    if np.linalg.det(P) == 0:\n",
    "        return isnotinverse\n",
    "    else:\n",
    "        return inverse\n",
    "#------------------------STATIONARY DISTRIBUTION-----------------\n",
    "# Answer each one with a True or False\n",
    "#A = isver(problem3_D)\n",
    "problem3_A_has_stationary = True\n",
    "problem3_B_has_stationary = True\n",
    "problem3_C_has_stationary = True\n",
    "problem3_D_has_stationary = True\n",
    "\n",
    "# Answer the following with the stationary distribution as a numpy array of shape (n_states,)\n",
    "# if the Markov chain has a stationary distribution otherwise answer with False\n",
    "#A = \n",
    "problem3_A_stationary_dist = getstationary(problem3_A)\n",
    "problem3_B_stationary_dist = getstationary(problem3_B)\n",
    "problem3_C_stationary_dist = getstationary(problem3_C)\n",
    "problem3_D_stationary_dist = getstationary(problem3_D)\n",
    "\n",
    "#print(problem3_A_stationary_dist.shape)\n",
    "#print(problem3_B_stationary_dist.shape)\n",
    "#print(problem3_C_stationary_dist.shape)\n",
    "#print(problem3_D_stationary_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "13"
   },
   "outputs": [],
   "source": [
    "# PART 5\n",
    "#------------------------REVERSIBLE-----------------\n",
    "# Answer each one with a True or False\n",
    "'''The reversibility of a Markov chain is a stricter form of irreducibility,\n",
    "i.e., it not only transfers between arbitrary states, but also transfers to each state with equal probability, \n",
    "so that a reversible Markov chain is a sufficient non-necessary condition for a smooth Markov chain.'''\n",
    "problem3_A_is_reversible = False\n",
    "problem3_B_is_reversible = False\n",
    "problem3_C_is_reversible = False\n",
    "problem3_D_is_reversible = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "lx_assignment_number": "vB",
  "lx_course_instance": "2023",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
